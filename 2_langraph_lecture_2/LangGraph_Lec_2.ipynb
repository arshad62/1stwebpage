{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arshad62/1stwebpage/blob/master/2_langraph_lecture_2/LangGraph_Lec_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What we will cover in this lecture:\n",
        "- Chaining\n",
        "- Router\n",
        "- Agents"
      ],
      "metadata": {
        "id": "Wl5q5JuPZKxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chaining\n",
        "\n",
        "## Goals\n",
        "\n",
        "Now, let's build up to a simple chain that combines 4 [concepts](https://python.langchain.com/v0.2/docs/concepts/):\n",
        "\n",
        "* Using [chat messages](https://python.langchain.com/v0.2/docs/concepts/#messages) as our graph state\n",
        "* Using [chat models](https://python.langchain.com/v0.2/docs/concepts/#chat-models) in graph nodes\n",
        "* [Binding tools](https://python.langchain.com/v0.2/docs/concepts/#tools) to our chat model\n",
        "* [Executing tool calls](https://python.langchain.com/v0.2/docs/concepts/#functiontool-calling) in graph nodes\n",
        "\n",
        "![Screenshot 2024-08-21 at 9.24.03 AM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbab08dd607b08df5e1101_chain1.png)"
      ],
      "metadata": {
        "id": "KOfOfagXcTAd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat Messages:\n",
        "\n",
        "## Messages in LangGraph:\n",
        "When you interact with a language model in LangGraph, messages are used to communicate between the user, the AI, and sometimes even tools or systems. Let's break it down step-by-step with an example.\n",
        "\n",
        "**1. Role (Who is speaking?)**\n",
        "- **User:** The person using the system (e.g., a student).\n",
        "- **Assistant:** The AI responding to the user.\n",
        "- **System:** Instructions that guide how the AI should respond.\n",
        "- **Tool:** A message from a tool the AI has used to help with the answer.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "- **User:** \"Tell me what photosynthesis is.\"\n",
        "- **Assistant:** \"Photosynthesis is the process by which plants convert sunlight into energy.\"\n",
        "- **System:** (You don't see this, but it might be something like \"Explain scientific terms clearly.\")\n",
        "- **Tool:** (If the AI used a dictionary or reference tool to check the definition, it might show the result here.)\n",
        "\n",
        "**2. Content (What is the message?)**: <br/>\n",
        "The content is the actual message or text. It can be a string (simple text) or more complex information, like a list or dictionary.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "- **User:** \"Define gravity.\"\n",
        "- **Assistant:** \"Gravity is a force that pulls objects toward each other, like the Earth pulling objects toward its surface.\"\n",
        "\n",
        "**3. Name (Who said it, if there are multiple people?):** <br/>\n",
        "- If there are multiple users or assistants, messages may have a name to tell them apart.\n",
        "**For example**, in a group chat with two students, you could have \"Student 1\" and \"Student 2.\"\n",
        "\n",
        "## Types of Messages (with Example):\n",
        "**1. HumanMessage:** <br/>\n",
        "This is the message from the user (you).\n",
        "\n",
        "**Example:**\n",
        "\n",
        "- **User:** \"Explain Newton's First Law of Motion.\"\n",
        "\n",
        "**2. AIMessage:**\n",
        "\n",
        "This is the message from the assistant (the AI), which may include extra details like:\n",
        "\n",
        "- **Response metadata:** Additional info like how many words the response used.\n",
        "- **Tool calls:** If the AI uses a tool, it shows which tool and what it sent to the tool.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "- **Assistant:** \"Newton's First Law of Motion states that an object will remain at rest or move at a constant speed in a straight line unless acted upon by an external force.\"\n",
        "- **Tool call:** If the AI used a science database to check the law, you might see a tool call like:\n",
        "    - **Tool name:** \"PhysicsAPI\"\n",
        "    - **Arguments:** \"Newton's First Law of Motion\"\n",
        "    - **ID:** \"tool12345\"\n",
        "\n",
        "**3. SystemMessage:** <br/>\n",
        "These are hidden instructions telling the AI how to respond.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "- The system might have a hidden message like \"Explain answers at a 6th-grade level.\"\n",
        "\n",
        "**4. ToolMessage:** <br/>\n",
        "This represents the result of a tool the AI has used to answer the question.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "- **ToolMessage:** \"The PhysicsAPI tool returned: 'Newton’s First Law means no force = no change in motion.'\"\n",
        "\n"
      ],
      "metadata": {
        "id": "6fJYWCIIlDVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langchain_google_genai langchain_core langgraph"
      ],
      "metadata": {
        "id": "FPnE62nAxfP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-EbzBqKY1C6",
        "outputId": "26fcf62f-22a1-4fd5-fbd7-ba3c8388d768"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: Model\n",
            "\n",
            "So you said you were researching ocean mammals?\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "Name: Lance\n",
            "\n",
            "Yes, that's right.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: Model\n",
            "\n",
            "Great, what would you like to learn about.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "Name: Lance\n",
            "\n",
            "I want to learn about the best place to see Orcas in the US.\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "\n",
        "messages = [AIMessage(content=f\"So you said you were researching ocean mammals?\", name=\"Model\")]\n",
        "messages.append(HumanMessage(content=f\"Yes, that's right.\",name=\"Lance\"))\n",
        "messages.append(AIMessage(content=f\"Great, what would you like to learn about.\", name=\"Model\"))\n",
        "messages.append(HumanMessage(content=f\"I want to learn about the best place to see Orcas in the US.\", name=\"Lance\"))\n",
        "\n",
        "for m in messages:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat Models\n",
        "\n",
        "### What are Chat Models?\n",
        "\n",
        "Chat models are a type of language model that take a series of **messages** as input and return a message as a response. Unlike older models (LLMs), chat models can handle different **roles** in a conversation:\n",
        "- **User**: The person asking the question (e.g., a student).\n",
        "- **Assistant**: The AI that provides answers.\n",
        "- **System**: Instructions that guide how the AI responds.\n",
        "\n",
        "**Example**:\n",
        "- **User**: \"What is the capital of France?\"\n",
        "- **Assistant**: \"The capital of France is Paris.\"\n",
        "\n",
        "### How They Work\n",
        "If you provide a plain text input, LangChain automatically converts it into a **HumanMessage** and processes it through the chat model. So, you can use chat models similarly to older LLMs.\n",
        "\n",
        "\n",
        "\n",
        "### Key Parameters\n",
        "\n",
        "When setting up a chat model, you can control its behavior with a few important settings:\n",
        "- **model**: The name of the model (e.g., GPT-4).\n",
        "- **temperature**: Controls how creative the model is. Higher values = more creative responses.\n",
        "- **max_tokens**: Limits the number of words the model generates.\n",
        "- **stop**: Defines where the model should stop responding.\n",
        "- **api_key**: Used to access the model from a third-party provider.\n",
        "\n",
        "**Example**:\n",
        "- **User**: \"Tell me a creative story about a cat.\"\n",
        "- **Assistant** (with higher temperature): \"Once, a curious cat explored space, chasing stars across the sky.\"\n",
        "\n",
        "\n",
        "\n",
        "### Multimodal Inputs\n",
        "\n",
        "Some chat models can take more than just text, like images or video. For example, **Gemini** can accept videos as inputs, but this feature is still rare.\n",
        "\n",
        "**Example**:\n",
        "- You send an image of a sunset, and the model describes it: \"This is a beautiful sunset over the mountains.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "ciQLERjS0Rwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "google_api_key = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "9dT8IvQIxFOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/newkey.json\""
      ],
      "metadata": {
        "id": "PX7ugs-R2NAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "llm: ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", api_key=google_api_key)\n",
        "result = llm.invoke(messages)\n",
        "type(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "eyaIk40N3Mx5",
        "outputId": "2b571b30-fe63-42d5-a971-f797c468a02c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.messages.ai.AIMessage"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.messages.ai.AIMessage</b><br/>def __init__(content: Union[str, list[Union[str, dict]]], **kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/langchain_core/messages/ai.py</a>Message from an AI.\n",
              "\n",
              "AIMessage is returned from a chat model as a response to a prompt.\n",
              "\n",
              "This message represents the output of the model and consists of both\n",
              "the raw output as returned by the model together standardized fields\n",
              "(e.g., tool calls, usage metadata) added by the LangChain framework.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 141);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBY4ZKdo35V5",
        "outputId": "f5f48d9d-db31-4c77-ddf5-7af36a608cb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"That's a great question! You're in luck, as the US has some fantastic places to see orcas, also known as killer whales.  Here are a few of the best spots, with some key considerations:\\n\\n**1. The San Juan Islands, Washington:**\\n\\n* **Pros:** This is arguably the most reliable place in the US to see orcas in the wild. The Salish Sea, which includes the San Juan Islands, is home to a resident population of orcas that frequent the area year-round.\\n* **Cons:**  Whale watching tours are popular here, so expect crowds and potential for boat traffic.\\n\\n**2. Alaska:**\\n\\n* **Pros:**  Alaska offers a variety of orca viewing opportunities, from boat tours in Southeast Alaska to land-based viewing in Glacier Bay National Park.  You'll have a chance to see both resident and transient orcas.\\n* **Cons:**  Alaska is a vast and remote area, so travel can be expensive and time-consuming.  Weather can be unpredictable, especially in the fall and winter.\\n\\n**3. California:**\\n\\n* **Pros:**  Orcas can be spotted off the coast of California, particularly in the Monterey Bay area.\\n* **Cons:**  Orca sightings are less frequent and predictable in California compared to the San Juan Islands or Alaska.\\n\\n**Key Considerations When Choosing a Destination:**\\n\\n* **Time of year:**  The best time to see orcas varies depending on the location. For example, the San Juan Islands are best visited during the summer months, while Alaska is best in the summer and fall.\\n* **Type of orca:**  Do you want to see resident orcas (those that stay in a specific area) or transient orcas (those that travel long distances)?\\n* **Budget:**  Whale watching tours can be expensive, so factor that into your budget.\\n* **Experience:**  Are you looking for a casual whale watching experience or a more immersive adventure?\\n\\nI hope this helps!  Let me know if you have any other questions. \\n\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-e1883167-cea4-4acd-a90b-efb42efd775c-0', usage_metadata={'input_tokens': 46, 'output_tokens': 428, 'total_tokens': 474})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.response_metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlqidHin372w",
        "outputId": "83c7594d-2220-41b5-93a7-4998c2107552"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'prompt_feedback': {'block_reason': 0, 'safety_ratings': []},\n",
              " 'finish_reason': 'STOP',\n",
              " 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT',\n",
              "   'probability': 'NEGLIGIBLE',\n",
              "   'blocked': False},\n",
              "  {'category': 'HARM_CATEGORY_HATE_SPEECH',\n",
              "   'probability': 'NEGLIGIBLE',\n",
              "   'blocked': False},\n",
              "  {'category': 'HARM_CATEGORY_HARASSMENT',\n",
              "   'probability': 'NEGLIGIBLE',\n",
              "   'blocked': False},\n",
              "  {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT',\n",
              "   'probability': 'NEGLIGIBLE',\n",
              "   'blocked': False}]}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools\n",
        "\n",
        "### What is a Tool?\n",
        "\n",
        "In LangChain, **tools** are used when a language model needs to interact with an external system, like an API. Tools help the model understand how to communicate with these systems by providing the necessary input formats, such as schemas or payloads, instead of just natural language.\n",
        "\n",
        "For example, if you want the model to get weather information from a weather API, you would bind the API as a **tool**. The model will then know how to format its request according to the API's requirements and return the output in the correct format.\n",
        "\n",
        "### How Tools Work:\n",
        "1. **User Input**: The user asks the model for something, like \"What's the weather in New York?\"\n",
        "2. **Tool Call**: The model recognizes that it needs to use a tool (the weather API) and formats the request based on the tool's schema.\n",
        "3. **Tool Output**: The API returns the response, which the model then delivers back to the user.\n",
        "\n",
        "### Example:\n",
        "\n",
        "Let’s say we bind a weather API as a tool. The model will use it like this:\n",
        "\n",
        "- **User**: \"What's the weather in New York?\"\n",
        "- **Model (via tool)**: Calls the weather API with the correct schema.\n",
        "- **Tool output**: \"The weather in New York is 75°F and sunny.\"\n",
        "  \n",
        "### Using Tools in LangChain:\n",
        "\n",
        "In LangChain, integrating tools is simple. You can bind a Python function or external system to the model using `ChatModel.bind_tools()`.\n",
        "\n",
        "```python\n",
        "from langchain_core.models import ChatModel\n",
        "\n",
        "# Example of a tool (Python function)\n",
        "def get_weather(location):\n",
        "    return f\"The weather in {location} is 75°F and sunny.\"\n",
        "\n",
        "# Bind the tool to the model\n",
        "model = ChatModel.bind_tools(get_weather)\n",
        "\n",
        "# Use the tool in a conversation\n",
        "user_input = \"What is the weather in New York?\"\n",
        "output = model.run(user_input)\n",
        "print(output)  # The weather in New York is 75°F and sunny.\n",
        "```\n",
        "\n",
        "### Why Tools are Important:\n",
        "- **Extend model capabilities**: Tools allow language models to fetch real-time information or perform actions that go beyond text generation.\n",
        "- **Schema-based communication**: Tools ensure that models interact with external systems using the right format, making it more reliable.\n",
        "\n",
        "Many LLM providers support tool-calling, and LangChain makes it easy to bind tools to models and integrate them into conversations. This is particularly useful when dealing with APIs or other structured data formats.\n",
        "\n",
        "[Many LLM providers support tool calling](https://python.langchain.com/v0.1/docs/integrations/chat/) and [tool calling interface](https://blog.langchain.dev/improving-core-tool-interfaces-and-docs-in-langchain/) in LangChain is simple.\n",
        "\n",
        "You can simply pass any Python `function` into `ChatModel.bind_tools(function)`.\n",
        "\n",
        "![Screenshot 2024-08-19 at 7.46.28 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbab08dc1c17a7a57f9960_chain2.png)"
      ],
      "metadata": {
        "id": "7MUsAXQw4cBH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This `multiply` function is our tool call"
      ],
      "metadata": {
        "id": "hj2y-XQ26zTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a * b\n",
        "\n",
        "llm_with_tools = llm.bind_tools([multiply])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "benEOBys4W5g",
        "outputId": "5ec67f5a-d937-461f-a01a-9f0e559d1272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we pass an input - e.g., `\"What is 2 multiplied by 3\"` - we see a tool call returned.\n",
        "\n",
        "The tool call has specific arguments that match the input schema of our function along with the name of the function to call.\n",
        "\n",
        "```\n",
        "{'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}\n",
        "```"
      ],
      "metadata": {
        "id": "TmvmSvzY9Kqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "function_call = llm_with_tools.invoke([HumanMessage(content=f\"What is 2 multiplied by 3\", name=\"Lance\")])\n",
        "function_call"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpuwUflr9K8F",
        "outputId": "3a8825d5-8e63-4397-dbd0-cfe1448919ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'multiply', 'description': 'Multiply a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int', 'parameters': {'type_': 6, 'description': 'Multiply a and b.\\n\\nArgs:\\n    a: first int\\n    b: second int', 'properties': {'a': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}, 'b': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['a', 'b'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'name': 'multiply', 'arguments': '{\"a\": 2.0, \"b\": 3.0}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-cb22bb89-f01a-4368-8f89-a58ecfa288e7-0', tool_calls=[{'name': 'multiply', 'args': {'a': 2.0, 'b': 3.0}, 'id': 'e7ff741f-6565-42f0-85ec-9cf317a8d5bf', 'type': 'tool_call'}], usage_metadata={'input_tokens': 71, 'output_tokens': 18, 'total_tokens': 89})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "function_call.additional_kwargs['function_call']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88YBqO0j9g1H",
        "outputId": "77719f2d-0ca7-4e89-9735-0e9ad2bd927e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'multiply', 'arguments': '{\"a\": 2.0, \"b\": 3.0}'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using messages as state\n",
        "\n",
        "With these foundations in place, we can now use [`messages`](https://python.langchain.com/v0.2/docs/concepts/#messages) in our graph state.\n",
        "\n",
        "Let's define our state, `MessagesState`, as a `TypedDict` with a single key: `messages`.\n",
        "\n",
        "`messages` is simply a list of messages, as we defined above (e.g., `HumanMessage`, etc)."
      ],
      "metadata": {
        "id": "FOHp_xklAs0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict\n",
        "from langchain_core.messages import AnyMessage\n",
        "\n",
        "class MessagesState(TypedDict):\n",
        "    messages: list[AnyMessage]"
      ],
      "metadata": {
        "id": "0x_w1Bap9rFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reducers\n",
        "\n",
        "Now, we have a minor problem!\n",
        "\n",
        "As we discussed, each node will return a new value for our state key `messages`.\n",
        "\n",
        "But, this new value will [will override](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers) the prior `messages` value.\n",
        "\n",
        "As our graph runs, we want to **append** messages to our `messages` state key.\n",
        "\n",
        "We can use [reducer functions](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers) address this.\n",
        "\n",
        "Reducers allow us to specify how state updates are performed.\n",
        "\n",
        "If no reducer function is specified, then it is assumed that updates to the key should *override it* as we saw before.\n",
        "\n",
        "But, to append messages, we can use the pre-built `add_messages` reducer.\n",
        "\n",
        "This ensures that any messages are appended to the existing list of messages.\n",
        "\n",
        "We annotate simply need to annotate our `messages` key with the `add_messages` reducer function as metadata."
      ],
      "metadata": {
        "id": "ZSoqQC_7BINl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "class MessagesState(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], add_messages]"
      ],
      "metadata": {
        "id": "w_OHIN3DBFMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since having a list of messages in graph state is so common, LangGraph has a pre-built [`MessagesState`](https://langchain-ai.github.io/langgraph/concepts/low_level/#messagesstate)!\n",
        "\n",
        "`MessagesState` is defined:\n",
        "\n",
        "* With a pre-build single `messages` key\n",
        "* This is a list of `AnyMessage` objects\n",
        "* It uses the `add_messages` reducer\n",
        "\n",
        "We'll usually use `MessagesState` because it is less verbose than defining a custom `TypedDict`, as shown above."
      ],
      "metadata": {
        "id": "TQEnFCysCXWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import MessagesState\n",
        "\n",
        "class MessagesState(MessagesState):\n",
        "    # Add any keys needed beyond messages, which is pre-built\n",
        "    pass"
      ],
      "metadata": {
        "id": "tuMrLxW4CRC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To go a bit deeper, we can see how the `add_messages` reducer works in isolation."
      ],
      "metadata": {
        "id": "tJHb2G3hCgEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial state\n",
        "initial_messages = [AIMessage(content=\"Hello! How can I assist you?\", name=\"Model\"),\n",
        "                    HumanMessage(content=\"I'm looking for information on marine biology.\", name=\"Lance\")\n",
        "                   ]\n",
        "\n",
        "# New message to add\n",
        "new_message = AIMessage(content=\"Sure, I can help with that. What specifically are you interested in?\", name=\"Model\")\n",
        "\n",
        "# Test\n",
        "add_messages(initial_messages , new_message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJlm82_5CbqZ",
        "outputId": "fb894734-f553-4010-c9f4-48966c104572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[AIMessage(content='Hello! How can I assist you?', additional_kwargs={}, response_metadata={}, name='Model', id='2aa6291e-b7e2-4e50-bbeb-7879f4952684'),\n",
              " HumanMessage(content=\"I'm looking for information on marine biology.\", additional_kwargs={}, response_metadata={}, name='Lance', id='5ab70ee6-aab1-49c4-847b-7d8d6015b360'),\n",
              " AIMessage(content='Sure, I can help with that. What specifically are you interested in?', additional_kwargs={}, response_metadata={}, name='Model', id='6016ad35-ba66-46bb-bd09-bc67cd7b6506')]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Our graph\n",
        "\n",
        "Now, lets use `MessagesState` with a graph."
      ],
      "metadata": {
        "id": "gFNXtR02DdzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "# Node\n",
        "def tool_calling_llm(state: MessagesState):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "# Build graph\n",
        "builder = StateGraph(MessagesState)\n",
        "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
        "builder.add_edge(START, \"tool_calling_llm\")\n",
        "builder.add_edge(\"tool_calling_llm\", END)\n",
        "graph = builder.compile()\n",
        "\n",
        "# View\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "FAEQcWrwDT64",
        "outputId": "5edbfa98-1646-4cdf-aaa8-5ec827da6749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/4gHYSUNDX1BST0ZJTEUAAQEAAAHIAAAAAAQwAABtbnRyUkdCIFhZWiAH4AABAAEAAAAAAABhY3NwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAA9tYAAQAAAADTLQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAlkZXNjAAAA8AAAACRyWFlaAAABFAAAABRnWFlaAAABKAAAABRiWFlaAAABPAAAABR3dHB0AAABUAAAABRyVFJDAAABZAAAAChnVFJDAAABZAAAAChiVFJDAAABZAAAAChjcHJ0AAABjAAAADxtbHVjAAAAAAAAAAEAAAAMZW5VUwAAAAgAAAAcAHMAUgBHAEJYWVogAAAAAAAAb6IAADj1AAADkFhZWiAAAAAAAABimQAAt4UAABjaWFlaIAAAAAAAACSgAAAPhAAAts9YWVogAAAAAAAA9tYAAQAAAADTLXBhcmEAAAAAAAQAAAACZmYAAPKnAAANWQAAE9AAAApbAAAAAAAAAABtbHVjAAAAAAAAAAEAAAAMZW5VUwAAACAAAAAcAEcAbwBvAGcAbABlACAASQBuAGMALgAgADIAMAAxADb/2wBDAAMCAgMCAgMDAwMEAwMEBQgFBQQEBQoHBwYIDAoMDAsKCwsNDhIQDQ4RDgsLEBYQERMUFRUVDA8XGBYUGBIUFRT/2wBDAQMEBAUEBQkFBQkUDQsNFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBT/wAARCADqAKEDASIAAhEBAxEB/8QAHQABAAIDAQEBAQAAAAAAAAAAAAUGAwQHCAIBCf/EAE8QAAEDAwEDBggKBwUGBwAAAAEAAgMEBREGBxIhExQVMZTTCBYiQVFUVmEXIzI0VXFydLHRNkKBkpWhsiQmN3OTCSVDYpHSMzVEUlNlpP/EABoBAQEAAwEBAAAAAAAAAAAAAAABAgMGBAX/xAAyEQEAAQIBCQYGAwEBAAAAAAAAAQIRAwQSFCExUXGR0UFSYWKSoQUTIzOxwVPh8DKB/9oADAMBAAIRAxEAPwD+qaIiAiIgItC9XiGx0LqmVkkziQyKnhAMk0h+SxgJAyT6SAOJJABIhhpOXUDeW1LLzrfH/lcMhFHFx6jwBlPmLn8D5mtzhbaaImM6qbR/ti2TE+obVSyFk1zo4njra+oY0/8AQlY/Gqy/TFB2ln5rHDo6wU7d2Kx22NvXhlJGB+CyeKtl+h6DszPyWf0fH2XUeNVl+mKDtLPzTxqsv0xQdpZ+aeKtl+h6DszPyTxVsv0PQdmZ+SfR8fY1HjVZfpig7Sz808arL9MUHaWfmnirZfoeg7Mz8k8VbL9D0HZmfkn0fH2NR41WX6YoO0s/NfUepLRM8NjutFI49QbUMJ/FfPirZfoeg7Mz8l8S6RsUzCySy26Rh62upIyD/JPo+PsmpLAggEHIK/VWDomK0/HacmNkmBLuax8aOX/lfF1NHvj3Xe8jgZSxXoXiCUSQOo66nfydTSPOTE/3H9ZpHFrvOD5jkDGqiLZ1E3j3LbkmiItKCIiAiIgIiICIiCsDF32gvbJh0NmpGSRtOeE8xeC70ZEbMA+iR3pVnVYto5ntBvcT8jntHTVMZxwduGSN4z6R8X+8FZ16MbbTEbLR+Lz73WRERedHL4PCR0JdaDUNRZbpNeZLLR1FbKymoKosmZC7ceYpOSLZQHkNJjL8ZWlobwltL6l2Q27Xd1NZZaaWCm53DJbas8nUTMa4RQ5hDqgZdgPia5rvMVzbZZbr7TapvGk9IWTVdn2dVVorzLbNW2808VqrnvxHHRTO8qSN+/ISwF7W4BDhnC0bTqTWVN4Oug9NUGnta6dqdPvtlo1S6jtMja4UbIXxzOoTunlfjIo8vh3iGPyOPUHcovCA2fy6AqtajUcLNNUdUyhqqySCZhpp3SMjEcsZYJIzvSMzvNGA4E4HFVLVvhX6X07eNGQU1Ldq63X6tqaWWrbZbgHwsip3S78cQpy6beduAbo+SXOGQ0kcRl0Jea3Qu1u30ulNXOo7rqnT1yoIb9BPU1dXSiekbLI5zy9ziOQkc5rjvMZu74b1Lu+3+C42vVey7V1LZbnfbfp69TyXCns9K6qqmRTUU8AkbE3ynhr3tzugkA5wg7FTzsqqeKaPe5ORoe3faWnBGRkEAg+48VkWra69t1tlHWtgnpm1MLJhDVRGKWMOaDuvYeLXDOCDxByFtICrF5xadZWOuZutFyL7ZUdeX4Y+aEn7JZKB/mlWdVjVTeeag0rRtyXitfWPwMgRxwvBOfN5ckY/avRgf9THhP4lYWdERedBERAREQEREBERBDais81caWuoDGy7UDi+nMpIZI1ww+J5GSGuHnwd1wa7Dt3BxxVlo1vaq62VlKyZksToK+0XCNpe1jgWuZLGcgtIyMjLXDiC4EFTqir1pi2agMbq2m3poxiOphkdDPGPOGysIe3zdRHUt1NVNURTX2dq8VMZ4NuyiNwc3ZvpZrgcgi0wAg/ur6h8HHZVTTRyxbOdLxyxuDmPbaYAWkcQQd1Tx0PI0nktS36Fp/V50x+P2vY4/wA08Saj2qv3+tD3Sy+Xh9/2ktG9aEVX8Saj2qv3+tD3Sxz6MqY4JHjVV+y1pIzND6P8pPl4ff8AaS0b1sRcv2WWu66w2Y6Qv1x1TeBcLpZ6OuqRTywiPlZIWPfu/FnycuOOJ4edWjxJqPaq/f60PdJ8vD7/ALSWjeibtsB2aX651VxuWgdOV9wqpHTT1VRa4XySvccuc5xbkkniSVrP8G7ZTIcv2caXcQAMm0wHgBgD5PoCn/Emo9qr9/rQ90niRK4bsmp79I09Y5xG3+bYwf5p8vD7/tJaN7Zgj0/s4sFJb6KlpbPbYcx0luoYQwEkl25FEweUSSTutGeJX7YbZUyV9RerlEIbhUsEMdMHB3NYGklrCQSC8k7zy3hnDQXBgcc1n0la7JUOqaeB8tY4EOq6uZ885B6xvvJcB7gQPcphSaqaYmmjt7TgIiLQgiIgIiICIiAiIgIiICIiAsVX81m+w78FlWGr+azfYd+CCk7BC07C9nJYSWnTduwSMEjmsfvP4n61fFRNgmfgM2dZLSfFy3ZLAAPmsfVu8MfVwV7QEREBERAREQEREBERAREQEREBERAREQFhq/ms32Hfgsyw1fzWb7DvwQUjYCANhGzgBzXgabtvlMGAf7LHxHAcP2K+qhbAMfARs33SS3xatuCW7v8A6WPzeZX1AREQEREBERAREQEREBF+OcGNLnENaBkkngAqUdYXu7AVFltlCba/jDUXCpfHJM3zPEbYzutPWMnJHWAt2HhVYt83otrrsipHTusPULH2ubu06d1h6hY+1zd2t2i1745wWXdFSOndYeoWPtc3dp07rD1Cx9rm7tNFr3xzgsu6KkdO6w9Qsfa5u7Tp3WHqFj7XN3aaLXvjnBZd1xjwpdvlf4O2hqXUsWk3aotktRzSrdHXc2dSlw+LcRyb95pIcCeGDu9e9wuHTusPULH2ubu1XNo1gvu0/Q170rebZZJLbdaZ1NLiqlLmZ4te3MXymuDXD3tCaLXvjnBZQ/AV29TbadmDaAaXlsdDpWjobRFXPqhK2ueyEteWtEbAzdDGHAz/AOIOrHH0suGbDtnV52E7NrXo+zUdlqIKQOfNVyVErX1Mzjl8jgI+s8B7gAPMr507rD1Cx9rm7tNFr3xzgsu6KkdO6w9Qsfa5u7Tp3WHqFj7XN3aaLXvjnBZd0VI6d1h6hY+1zd2nTusPULH2ubu00WvfHOCy7oqR07rD1Cx9rm7tOndYeoWPtc3dpote+OcFl3RUpup9S0Xx1baKCppm8ZG2+qkdMG+cta6MB56+GR1cMngrdQ1sFyooKumkE1NPG2WKRvU5rhkEfWCtOJg14eurqWZ0RFpRF6oJbpm7kHBFHMQR9gqvaZAGm7UAAAKSLAH2ArDqr9GLx9zm/oKr2mv0ctX3SL+gL6OD9meP6XsSSItGz3y36houeWutguFJyskPL00gezfjeWPbkcMtc1zT6CCskbyIsVXVRUNLNUzu3IYWOke7BOGgZJwOPUEGVFG6Z1HbtYaett8tFRzu13GnZVUs+45nKRPaHNduuAcMgjgQCpJARFD6i1dadJutTbrV81N0ro7bRjk3v5WoeHFjPJBxkMdxOBw4lQTCKvR7QLBLNqeJlfvSaaIF1byMn9mzCJx+r5fxbmu8je68dfBStlvFJqGz0N0oJeXoK6COpp5d1zd+N7Q5rsOAIyCDggH0oNxERUEUPctXWm0ajs1iq6vkrreGzuoafk3u5YQta6XygC1uA5p8ojOeGU0zq606xpayps9XzyGjrZ7fO7k3s3J4XmOVmHAZ3XNIyOBxwJCgmERFQWDZac7PbF7qYAe4ZOFnWDZb/h7Yvu4/EqYv2J4x+JXsWpERfNRF6q/Ri8fc5v6Cq9pr9HLV90i/oCsOqv0YvH3Ob+gqvaa/Ry1fdIv6Avo4P2Z4/pexvVEIqYJInOexsjS0ujcWuGRjII4g+8Lx1okVmzDwVr5f9P3m6012q73UWp1VW3GaqhoGPvL6d07IpHFjHhjy4uABc7ynZ4r2SqJTbDdD0ldqGpjsMf8AeBkrLnTPnlfTVAlIdIeQLzG1zi0Eua0EnzqTF0cQ2kam1B4PF81Hb9O6hvWpIpNFV17bBfq11fJRVVPJGxlQHPyQxwldlnySY+AHFSFypblsx1RoagpdY33VFNq+1XOK5w3ivdVskdFRGdtVCHcIRvDdLWYbiRoxkBdj0dsV0XoMXLoeyMa64wimq5K2olrJJYQCBEXzPe7k8E+QDu8epYtE7DND7O7o+42GxNpa10BpWyzVM1QYYScmKISvcImZA8hm6OA4cFM2Ro+DQQfB62b4Of7v0XV/ktUVt1vF0qNVbOdGUV5q9OW/U9xqIq+50EgiqBHDTPmbBHJjyHSOAG8PKwDjrUxS7N7xoOihtOzep0/pqwNL5nUNyttTXFsr3EuMZFXGGM6sMAwDnHXhbFVs3qdfaeqbRtL6D1NSmaOemFsoJ6EwPbnDw41Ejw8Z4OY5pAyOOVddrChbTLGNI2nTWh7VeNbX293u4zT0MbNSPppjHFDmUTVpDpGwMBa7A3nlxAGRkLnlo1BfrxorQNHqSrkrLjZNrAtHL1FTzmUxxGcMa+bdZyrmh27vlrS7AJAJXeHeDzoF9jp7UbLMKenrHV8U4uVUKpk7mBjnioEvK5cwBp8vBAAPUsg8H3Z83Stdppmm4YrHW1bK+WjinlY1tS0NDZoyHgxvwxvlMLSTknJJJxzZuOXVcEk9T4T/ACNdW26oglp6mKpt1U+mnjfHaYJGFsjCHN8pgzg8RkHgStXSNJddpOvNNWa5at1LR25+za03OWO2XWWmdLVvllaZ3Pad4vx1nPlYG9vYAHcqvZVpWtvl8vEtpb0lfLf0Xcp45pGc6p93d3XhrgC4N4B+N4DgCAtix7OtPabu1Lc7db+b11NaYbHFLy0jt2jicXRxYc4g4Lid4jeOeJKuaPLds2lXTUmmtlMmu9V3+x6auFjruWu1iklhnrbpDM2ONsskLS8ZibI8NGA92c5xhegPByp9QU+xXS41Sa436SGSapdcnvdUu35XvaZN8ktcWOaS39UndAAGBT9pXg8Gqt2lqDRdjsvMLLDUwMhuN6ulBNGyV7XkMnpnlzmlzSS14dk7uC3CmNDaB2p6L0jbbRHrSwVr4GyGSS7WurrpGl0r3iNsxrGOcxjXNY0vBdhuSeOBIiYnWP3aQceEVsb98N8H/wCeFclhuNyodlNdFartW2Wet2uT2+Wqt8vJzNimu7mPAPEcWuPAgj0grv7dnMuq2Wuo18LRfbtZ7g2vtdXaaSeg5q9oGOueRxJIORvbrhgFpxx+37FNFvrrhV9CMjmr7rT3up5KolYyStgfvxT7jXhodvcTgAPPyg5W0yODXjS90o73tst1NrvWUVJpOz011s7TfZ3ugqJKaaRxe9xL5Wb0DcRyFzQHO4ceHpDZ7eqjUmgNM3esINXX2ymqpi0YBe+JrnYHm4krHUbOtPVVZqeqlt+9PqWljorq/lpBzmFjHxsbjewzDZHjLMHj15AUxZ7RSWC0UNroIuQoaKBlNTxbxduRsaGtbkkk4AAySSrEWG4sGy3/AA9sX3cfiVnWDZb/AIe2L7uPxKyxfsTxj8SvYtSIi+aiL1V+jF4+5zf0FV7TX6OWr7pF/QFcaiCOqgkhlbvxSNLHNPnBGCFQ4aW/6Zp4bc2yTXynp2NihrKOoha57AMN5Rsr2YfgccEg9fDO6PoZPMTRNF7Te+ubfllGuLJ1FCdLX72MuvaqLv06Wv3sZde1UXfrfmeaPVHUsm0UJ0tfvYy69qou/Tpa/exl17VRd+mZ5o9UdSybRQnS1+9jLr2qi79Olr97GXXtVF36Znmj1R1LJtFCdLX72MuvaqLv1+PvN9jY5ztG3UNaMk85o+/TM80eqOpZOIqpp7Wtw1XYLZe7XpO61NsuVLFWUs/L0jOUikYHsduumBGWuBwQCPOFIdLX72MuvaqLv0zPNHqjqWTaKE6Wv3sZde1UXfp0tfvYy69qou/TM80eqOpZNooTpa/exl17VRd+nS1+9jLr2qi79MzzR6o6lk2ihOlr97GXXtVF36dLX72MuvaqLv0zPNHqjqWTawbLf8PbF93H4lRrajUlw+Ig07LbJH8OdXGpgdHF/wA27FI9ziOJDeGSMFzc5FusdohsFmorbTue6GlhbC10hy52BjJPnJ6z7ytOPMU4eZeJmZidUxOy+7ibIbyIi+cxEREBERAREQEREBYqv5rN9h34LKsNX81m+w78EFM2Ejd2IbPRjdxp23DG7u4/s0fmwMf9B9QV5VE2CM5PYXs6YGuaG6ctw3XN3SP7LHwIycfVlXtAREQEREBERAREQEREBERAREQEREBERAWGr+azfYd+CzLDV/NZvsO/BBSNgJB2EbOC05adN23BLQ3I5rH5h1fUr6qLsGDxsN2diQyGTxct28ZRh5PNo87w9PpV6QEREBERAREQEREBERAREQEREBERARRN31bY9PyNjud5oLdI7GGVVSyNxz1YDiCVF/Cro32qs/bY/wA1upwMWuL00zMcJW0rUq/q/W+ndHUrOn7/AGux85Y8QdJVkdPyu6Bvbu+4b2N5ucdWR6Vq/Cro32qs/bY/zXD/AAx7BovbzsRu1ppNR2aXUFv/AN42l3PY94zsBzGDn/iNLm46slpPUstGx+5PKVzZ3Ok+DXquw6h2L6Jo7Pebfc6i3aftsVXBRVUcr6Z3NmgNka1ziw5Y4YJ/VPXgrqK8bf7P/SmmNiWyGauvl9tlBqjUcraqtpp6tjJKeJm82CJzSeDgHPcfRymD1L0/8Kujfaqz9tj/ADTRsfuTykzZ3LUiqvwq6N9qrP22P81v2rW+nb7PyFtv1tr5/wD4qarjkf8Aug5UnAxaYvVRMRwlLSm0RFoQREQEREBERAREQEREBcZ2jbTKq51s9osdTJSUUDnRVNdA7dkmeOBZG4cWtacguHEkYGAMu6DtIvk2ndC3qvpn8nVMpyyB/wD7JXkMY79jnA/sXnqmp2UtPHDGMMjaGge4Lpvg+R0Yt8fEi9tUcV2Rd8wUcFM5zoomMe45c8DynH0k9ZPvKzIi7JgIq9q/W9HpAUUUlLW3O4Vz3MpbdbohJPNujLyASGhrRjLnEAZHHJCr0226yU9tgqZLfd21cl0bZ32zmg53DVOidKxr2b2MOa0Yc0keUDnGSNNWNh0zaqUdCRUWLbFY2WK+XK4QV9nksszKest9bCOcskeGmJrWsc4PL99u7uk5yorSG0e56q2sVlrkt9zslsiscVULfdaaOKXljO9vKAtLiQW4GN7AIPAHKx+fh3iInb/v0rp6xVFJBVtDZoY5QDkB7QcFZUXoFu0NtKqtJTx0t1qpauxHyTLUPL5KTj8reJyYx5wc7o4jgMLu4ORkcQvLLmh7S1wDmkYIPUV27YxdpLnoKkimeZJaGSSiLz1lrHEMz79zc/auS+M5HRREZRhxbXaevVltheURFygIiICIiAiIgIiIKptUtk132e3yCna6SdtPy8cbPlPdGRIGj3ksx+1cCilZPEySNwcx4DmuHnB6l6oXBNoWgJdF1M1dRRGTT8jnSEsb8yJOS1wH/D68O6m9RwACep+C5VRRfJ65tebx06G2HN71r7TGm6zml31HabXV7ofyFbXRQybp6juucDjgeK0Phd0L7aae/isH/erNyVPVBsu5FMHDg/AdkfWnMab1eL9wLqpjEvqmOX9sXH9o9hoNpt305qewUlm2h0NndUUlZaW1cEjJGytYd5j3EsEjC1pw4jId1hZTs+qJLfo+W1aKo9KOptSxXCtoaSWDyIGQzMEryzDXO8to3W7xGfPxXX44mQgiNjWA8cNGF9LRo1M1TVVtnh4f+9m9HE9Y7NNR3bUOsbrQUkTpulLPdrXHPM1sdY6lYN+NxBJZkgjLgOOPNxUja7jcqLaXV6x1bbINF2Z1kitrJblc6ZwMwqHv3SWvwODuHpx6eA62vmSNkrd17Wvb6HDIV0aIqzqZnbfwveZ/aqr8LmhTn++mnuH/ANrB/wB62rXtI0le6+Kht2qbLX1sxIjpqW4QySPIBJw1riTwBPD0Kc5lT+rxfuBfraaCI74ijYRx3g0DC3RGJfXMcv7RlXaNiFA+l0FDUvBbz+omrGA+djnYYfqLGtd+1cz0XomfaBOMb8Vh4iorW8OVGcGOI+cniC4cG+nK9Dwwx00McMTGxRRtDGMYMNaAMAAeYLmvjWVUTTGT0zeb3nw8GcaofaIi5EEREBERAREQEREBERBUbpsm0nd6h1RNZo4JnO3nPopJKYuPpdyTm5P1qP8AgN0j6rX/AMWq+9V+ReynLMppi1OJVEcZW8qD8BukfVa/+LVfep8BukfVa/8Ai1X3qvyLLTsq/lq5yXlQfgN0j6rX/wAWq+9T4DdI+q1/8Wq+9V+RNOyr+WrnJeVB+A3SPqtf/FqvvVt0Gx3R9vmbKLOKp4II5/US1TQR1Hdlc4fyVzRYzluVVRacWrnJeXyxjY2NYxoa1owGgYAHoX0iLxoIiICIiAiIg//Z\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we pass in `Hello!`, the LLM responds without any tool calls."
      ],
      "metadata": {
        "id": "b0YC5alkDnEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = graph.invoke({\"messages\": HumanMessage(content=\"Hello!\")})\n",
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-3HpXD9DhfI",
        "outputId": "87f08181-af21-448d-8fc2-b66a59369dc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'multiply', 'description': 'Multiply a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int', 'parameters': {'type_': 6, 'description': 'Multiply a and b.\\n\\nArgs:\\n    a: first int\\n    b: second int', 'properties': {'a': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}, 'b': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['a', 'b'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Hello!\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello! 👋 How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = graph.invoke({\"messages\": HumanMessage(content=\"Multiply 2 and 3\")})\n",
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36v-OToODpVn",
        "outputId": "00249758-89e7-49ad-886f-6b5595e2ba0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'multiply', 'description': 'Multiply a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int', 'parameters': {'type_': 6, 'description': 'Multiply a and b.\\n\\nArgs:\\n    a: first int\\n    b: second int', 'properties': {'a': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}, 'b': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['a', 'b'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Multiply 2 and 3\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  multiply (5937c612-86b0-431c-9dde-255c423e005b)\n",
            " Call ID: 5937c612-86b0-431c-9dde-255c423e005b\n",
            "  Args:\n",
            "    b: 3.0\n",
            "    a: 2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Router\n",
        "\n",
        "### What is a Router in LangGraph?\n",
        "\n",
        "In LangGraph, a **Router** is a mechanism that decides whether a language model should respond directly to the user or call a tool to provide the answer. This decision is based on the user’s input, so the model can \"route\" between two actions:\n",
        "1. **Returning a natural language response**.\n",
        "2. **Calling a tool** (like an API) to gather more information before responding.\n",
        "\n",
        "### Example:\n",
        "\n",
        "Imagine you're interacting with an AI, and you ask:\n",
        "- **User**: \"What is the weather in New York?\"\n",
        "\n",
        "The **Router** can:\n",
        "- Either **return a direct response**: \"The weather is nice today.\"\n",
        "- Or **call a weather tool** to get real-time data from an API and return a more accurate answer: \"It’s 75°F and sunny in New York.\"\n",
        "\n",
        "### Goals of a Router:\n",
        "\n",
        "1. **Call a tool** when the user asks for external information.\n",
        "2. **Respond naturally** when the information is already available.\n",
        "\n",
        "\n",
        "### Extending the Graph:\n",
        "\n",
        "To make the router work effectively, you need to add two key components:\n",
        "\n",
        "1. **Node for calling the tool**: This node in the graph will represent where the model goes if it needs to call a tool, like a weather API.\n",
        "   \n",
        "2. **Conditional edge**: This edge will check the model’s output. If the model decides to call a tool, it routes the flow to the tool node; otherwise, it ends with a direct response.\n",
        "\n",
        "### Example in Action:\n",
        "\n",
        "1. **User**: \"What’s the capital of France?\"\n",
        "   - Since the model knows this answer, it returns a **direct response**: \"The capital of France is Paris.\"\n",
        "\n",
        "2. **User**: \"What’s the current weather in Paris?\"\n",
        "   - The router recognizes that it doesn’t have the current weather information, so it **calls the weather API tool** and returns: \"It’s 68°F and cloudy in Paris.\"\n",
        "\n",
        "\n",
        "### Visualization:\n",
        "\n",
        "Imagine a flowchart (like the one in the image):\n",
        "- At the top is the **user input**.\n",
        "- The **router** decides whether to follow the path to a **direct response** or to call a **tool**.\n",
        "- If it’s a tool call, the tool provides the needed information, and then the model gives the final response to the user.\n",
        "\n",
        "---\n",
        "\n",
        "### Who Decides the Path?\n",
        "\n",
        "The **LLM (Large Language Model)** doesn’t directly decide the path. Instead, it generates an output based on the user's input. The **edges** in LangGraph decide the path by interpreting that output and routing the conversation flow.\n",
        "\n",
        "### LLM's Role\n",
        "\n",
        "- The LLM processes the user's input and creates an **output**.\n",
        "- This output can either be a **direct response** or an indication that a tool (like an API) needs to be called.\n",
        "\n",
        "### Edges' Role\n",
        "\n",
        "- The edges evaluate the LLM's output and make the decision:\n",
        "  - If the LLM can answer directly, the edge routes the flow to a **response node**.\n",
        "  - If a tool is needed, the edge directs the flow to a **tool-calling node**.\n",
        "\n",
        "### Example\n",
        "\n",
        "1. **User**: \"What's the weather in New York?\"\n",
        "2. **LLM Output**: \"I need to call a weather API tool.\"\n",
        "3. **Edges**: Detect that a tool call is needed and route to the tool node.\n",
        "4. **Tool**: Calls the weather API and returns the information.\n",
        "\n",
        "### Summary\n",
        "\n",
        "- **LLM**: Generates the output.\n",
        "- **Edges**: Decide whether to respond directly or call a tool.\n",
        "\n",
        "The edges ensure that the flow of the conversation is properly managed based on the LLM's output."
      ],
      "metadata": {
        "id": "MyE-Gr5KL7SJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an simple example of an agent, where the LLM is directing the control flow either by calling a tool or just responding directly.\n",
        "\n",
        "![Screenshot 2024-08-21 at 9.24.09 AM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbac6543c3d4df239a4ed1_router1.png)\n",
        "\n",
        "Let's extend our graph to work with either output!"
      ],
      "metadata": {
        "id": "fixTsV2pRlt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a * b\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
        "llm_with_tools = llm.bind_tools([multiply])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihkSnq-WDumC",
        "outputId": "cb0bc48b-4931-4406-b56d-cca38f322961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " We use the [built-in `ToolNode`](https://langchain-ai.github.io/langgraph/reference/prebuilt/?h=tools+condition#toolnode) and simply pass a list of our tools to initialize it.\n",
        "\n",
        " We use the [built-in `tools_condition`](https://langchain-ai.github.io/langgraph/reference/prebuilt/?h=tools+condition#tools_condition) as our conditional edge."
      ],
      "metadata": {
        "id": "kF_VSJwrTATr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph import MessagesState\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.prebuilt import tools_condition\n",
        "\n",
        "# Node\n",
        "def tool_calling_llm(state: MessagesState):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "# Build graph\n",
        "builder = StateGraph(MessagesState)\n",
        "\n",
        "# Add Nodes\n",
        "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
        "builder.add_node(\"tools\", ToolNode([multiply]))\n",
        "\n",
        "# Add Edges\n",
        "builder.add_edge(START, \"tool_calling_llm\")\n",
        "builder.add_conditional_edges(\n",
        "    \"tool_calling_llm\",\n",
        "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
        "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
        "    tools_condition,\n",
        ")\n",
        "builder.add_edge(\"tools\", END)\n",
        "graph = builder.compile()\n",
        "\n",
        "# View\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "VtlzV78aS6Bh",
        "outputId": "6b2b3050-6e9e-4836-834d-bec968804254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "messages = [HumanMessage(content=\"Hello world.\")]\n",
        "messages = graph.invoke({\"messages\": messages})\n",
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u07s0gloVa7o",
        "outputId": "3841be72-28bd-41c2-90c5-261812a8d524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'multiply', 'description': 'Multiply a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int', 'parameters': {'type_': 6, 'description': 'Multiply a and b.\\n\\nArgs:\\n    a: first int\\n    b: second int', 'properties': {'b': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}, 'a': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['a', 'b'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Hello world.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello! How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "messages = [HumanMessage(content=\"Multiply 2 and 3.\")]\n",
        "messages = graph.invoke({\"messages\": messages})\n",
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leuNkvKSVmgz",
        "outputId": "3eb6cf82-0945-4b43-9b82-38348ba9585c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'multiply', 'description': 'Multiply a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int', 'parameters': {'type_': 6, 'description': 'Multiply a and b.\\n\\nArgs:\\n    a: first int\\n    b: second int', 'properties': {'b': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}, 'a': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['a', 'b'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Multiply 2 and 3.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  multiply (11e5d788-abc9-4f0d-ba51-b04d53dce316)\n",
            " Call ID: 11e5d788-abc9-4f0d-ba51-b04d53dce316\n",
            "  Args:\n",
            "    a: 2.0\n",
            "    b: 3.0\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: multiply\n",
            "\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agents:\n",
        "\n",
        "## Review\n",
        "\n",
        "We built a router.\n",
        "\n",
        "* Our chat model will decide to make a tool call or not based upon the user input\n",
        "* We use a conditional edge to route to a node that will call our tool or simply end\n",
        "\n",
        "![Screenshot 2024-08-21 at 12.44.33 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbac0ba0bd34b541c448cc_agent1.png)\n",
        "## Goals\n",
        "\n",
        "Now, we can extend this into a generic agent architecture.\n",
        "\n",
        "In the above router, we invoked the model and, if it chose to call a tool, we returned a `ToolMessage` to the user.\n",
        "\n",
        "But, what if we simply pass that `ToolMessage` *back to the model*?\n",
        "\n",
        "We can let it either (1) call another tool or (2) respond directly.\n",
        "\n",
        "This is the intuition behind [ReAct](https://react-lm.github.io/), a general agent architecture.\n",
        "  \n",
        "* `act` - let the model call specific tools\n",
        "* `observe` - pass the tool output back to the model\n",
        "* `reason` - let the model reason about the tool output to decide what to do next (e.g., call another tool or just respond directly)\n",
        "\n",
        "This [general purpose architecture](https://blog.langchain.dev/planning-for-agents/) can be applied to many types of tools.\n",
        "\n",
        "![Screenshot 2024-08-21 at 12.45.43 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbac0b4a2c1e5e02f3e78b_agent2.png)\n",
        "\n",
        "\n",
        "## What is the ReAct Agent Architecture?\n",
        "\n",
        "The **ReAct architecture** stands for **Reasoning and Acting**. It is designed to allow AI models to:\n",
        "\n",
        "1. **Act**: The model calls a specific tool (e.g., to look up information).\n",
        "2. **Observe**: It gets back the output from the tool.\n",
        "3. **Reason**: It then thinks about what to do next—whether to call another tool or provide a final response.\n",
        "\n",
        "The ReAct process helps the model decide how to continue based on the information it gathers.\n",
        "\n",
        "---\n",
        "\n",
        "### Core Steps in ReAct\n",
        "\n",
        "- **Act**: The model calls tools based on the task at hand.\n",
        "- **Observe**: The model gets the tool's output and understands what has changed or been learned.\n",
        "- **Reason**: The model analyzes the tool’s output and decides whether to call another tool or respond to the user.\n",
        "\n",
        "---\n",
        "\n",
        "### Example 1: Weather and Restaurant Recommendation\n",
        "\n",
        "Let’s start with an easy example:\n",
        "\n",
        "- **User**: “Can you tell me the weather in Paris and suggest a good café nearby?”\n",
        "\n",
        "**Step 1: Act**\n",
        "\n",
        "The agent will act by calling a **Weather API tool** to get the weather in Paris.\n",
        "\n",
        "- **Agent** calls the weather tool:\n",
        "  - **Tool Output**: “It’s 22°C and sunny in Paris.”\n",
        "\n",
        "**Step 2: Observe**\n",
        "\n",
        "The agent observes the tool's output and sees that it now has the weather information.\n",
        "\n",
        "**Step 3: Reason**\n",
        "\n",
        "The agent now reasons, thinking: \"Okay, I got the weather. But I also need to find cafés.\"\n",
        "\n",
        "- The agent **calls** another tool, such as **Google Maps** to find nearby cafés.\n",
        "  - **Tool Output**: “Here are three cafés: Café de Paris, Le Bistro, and La Terrasse.”\n",
        "\n",
        "Finally, the agent **responds** to the user with both pieces of information:\n",
        "\n",
        "- **Agent**: “It’s 22°C and sunny in Paris. Here are three cafés nearby: Café de Paris, Le Bistro, and La Terrasse.”\n",
        "\n",
        "---\n",
        "\n",
        "### Example 2: Meeting Planning with Traffic and Weather\n",
        "\n",
        "Let’s make it a bit more complex. Imagine you want to know if you'll be able to make it to your meeting on time given the current traffic and weather conditions.\n",
        "\n",
        "- **User**: “I have a meeting in two hours. Can you check the quickest route and tell me if it will rain?”\n",
        "\n",
        "**Step 1: Act**\n",
        "\n",
        "The agent will act by first using a **Maps API** to find the fastest route.\n",
        "\n",
        "- **Agent** calls the Maps API tool:\n",
        "  - **Tool Output**: “The fastest route to your meeting is via Main St. It will take 45 minutes.”\n",
        "\n",
        "**Step 2: Observe**\n",
        "\n",
        "The agent now observes that it has the route information but still needs weather details.\n",
        "\n",
        "**Step 3: Reason**\n",
        "\n",
        "The agent reasons that it also needs to know if it will rain, so it calls a **Weather API tool**.\n",
        "\n",
        "- **Tool Output**: “It will start raining in 1 hour.”\n",
        "\n",
        "Now the agent combines both outputs and gives you the final response:\n",
        "\n",
        "- **Agent**: “The fastest route to your meeting is via Main St. It will take 45 minutes, but it will start raining in 1 hour. You should leave soon and take an umbrella!”\n",
        "\n",
        "---\n",
        "\n",
        "## Key Points to Remember\n",
        "\n",
        "- **Act**: The agent makes decisions to call tools based on the query.\n",
        "- **Observe**: It gathers and processes tool outputs.\n",
        "- **Reason**: It evaluates the output and decides the next action—whether to call another tool or respond.\n",
        "\n",
        "---\n",
        "\n",
        "### How the ReAct Architecture Enhances AI Agents\n",
        "\n",
        "- **Efficiency**: The agent doesn't waste time or resources—it only calls tools as needed.\n",
        "- **Flexibility**: It can handle complex, multi-step queries like those we saw in the examples.\n",
        "- **Modularity**: The architecture is flexible enough to add new tools easily as the system evolves.\n",
        "\n",
        "---\n",
        "\n",
        "### Practical Application Exercise\n",
        "\n",
        "Now, let’s apply the ReAct architecture in a real-world case:\n",
        "\n",
        "1. **User**: “Can you find the nearest pharmacy and tell me if it’s open?”\n",
        "\n",
        "2. **Act**: The agent first calls a Maps API to find nearby pharmacies.\n",
        "   - **Tool Output**: “The nearest pharmacy is CityPharma.”\n",
        "\n",
        "3. **Observe**: The agent has the pharmacy name but needs to check if it’s open.\n",
        "\n",
        "4. **Reason**: The agent calls a **Business Hours API** to check the opening hours.\n",
        "   - **Tool Output**: “CityPharma is open until 8 PM today.”\n",
        "\n",
        "5. **Response**: “The nearest pharmacy is CityPharma, and it’s open until 8 PM today.”\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-iDhswZhhLHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langchain_google_genai langchain_core langgraph"
      ],
      "metadata": {
        "id": "SGaVGbQQV9c9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"GEMINI_API_KEY\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAXfys1cy4mz",
        "outputId": "19bbdba8-f2f3-470d-dfc3-e98fcf213d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GEMINI_API_KEY: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we'll use [LangSmith](https://docs.smith.langchain.com/) for [tracing](https://docs.smith.langchain.com/concepts/tracing).\n",
        "\n",
        "We'll log to a project, `langchain-academy`."
      ],
      "metadata": {
        "id": "2o9wjzzU0MBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "langsmith_key = userdata.get('LANGCHAIN_API_KEY')"
      ],
      "metadata": {
        "id": "BotIBEwp0lE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"LANGCHAIN_API_KEY\"] = langsmith_key\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-academy\""
      ],
      "metadata": {
        "id": "M7L-JixczW1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Obtain the necessary json key from the Google Cloud Console by following the instructions outlined in step 21_langchain_ecosystem/langchain/-01_gemini_standalone/Gemini_API_python.ipynb file. Once acquired, load the json key in Google Colab to proceed with the project."
      ],
      "metadata": {
        "id": "XrBWkFCq1Nq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/newkey.json\""
      ],
      "metadata": {
        "id": "C3s-XCCE0QtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a * b\n",
        "\n",
        "# This will be a tool\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Adds a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a + b\n",
        "\n",
        "def divide(a: int, b: int) -> float:\n",
        "    \"\"\"Divide a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a / b\n",
        "\n",
        "tools = [add, multiply, divide]\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
        "llm_with_tools = llm.bind_tools(tools)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gq8EEyBp3Nw4",
        "outputId": "a5ba33aa-5a21-4ecc-9052-815d960ce741"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create our LLM and prompt it with the overall desired agent behavior."
      ],
      "metadata": {
        "id": "pDj2udsV3fiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import MessagesState\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# System message\n",
        "sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\")\n",
        "\n",
        "# Node\n",
        "def assistant(state: MessagesState):\n",
        "   return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}"
      ],
      "metadata": {
        "id": "54txqV8P3fGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, we use `MessagesState` and define a `Tools` node with our list of tools.\n",
        "\n",
        "The `Assistant` node is just our model with bound tools.\n",
        "\n",
        "We create a graph with `Assistant` and `Tools` nodes.\n",
        "\n",
        "We add `tools_condition` edge, which routes to `End` or to `Tools` based on  whether the `Assistant` calls a tool.\n",
        "\n",
        "Now, we add one new step:\n",
        "\n",
        "We connect the `Tools` node *back* to the `Assistant`, forming a loop.\n",
        "\n",
        "* After the `assistant` node executes, `tools_condition` checks if the model's output is a tool call.\n",
        "* If it is a tool call, the flow is directed to the `tools` node.\n",
        "* The `tools` node connects back to `assistant`.\n",
        "* This loop continues as long as the model decides to call tools.\n",
        "* If the model response is not a tool call, the flow is directed to END, terminating the process."
      ],
      "metadata": {
        "id": "9CUCiOuN3wbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import START, StateGraph\n",
        "from langgraph.prebuilt import tools_condition\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# Graph\n",
        "builder = StateGraph(MessagesState)\n",
        "\n",
        "# Define nodes: these do the work\n",
        "builder.add_node(\"assistant\", assistant)\n",
        "builder.add_node(\"tools\", ToolNode(tools))\n",
        "\n",
        "# Define edges: these determine how the control flow moves\n",
        "builder.add_edge(START, \"assistant\")\n",
        "builder.add_conditional_edges(\n",
        "    \"assistant\",\n",
        "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
        "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
        "    tools_condition,\n",
        ")\n",
        "builder.add_edge(\"tools\", \"assistant\")\n",
        "react_graph = builder.compile()\n",
        "\n",
        "# Show\n",
        "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "Fk4L8HZM3qdB",
        "outputId": "e04ad1c0-caba-4bde-ff93-f6bbc7f83496"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [HumanMessage(content=\"Add 3 and 4. Multiply the output by 2. Divide the output by 5\")]\n",
        "messages = react_graph.invoke({\"messages\": messages})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7kARD0V5Cfy",
        "outputId": "b52e5307-aaa0-42ca-bcb3-6f4e8fb30acd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'add', 'description': 'Adds a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int', 'parameters': {'type_': 6, 'description': 'Adds a and b.\\n\\nArgs:\\n    a: first int\\n    b: second int', 'properties': {'b': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}, 'a': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['a', 'b'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'multiply', 'description': 'Multiply a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int', 'parameters': {'type_': 6, 'description': 'Multiply a and b.\\n\\nArgs:\\n    a: first int\\n    b: second int', 'properties': {'a': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}, 'b': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['a', 'b'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'divide', 'description': 'Divide a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int', 'parameters': {'type_': 6, 'description': 'Divide a and b.\\n\\nArgs:\\n    a: first int\\n    b: second int', 'properties': {'b': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}, 'a': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['a', 'b'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'add', 'description': 'Adds a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int', 'parameters': {'type_': 6, 'description': 'Adds a and b.\\n\\nArgs:\\n    a: first int\\n    b: second int', 'properties': {'b': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}, 'a': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['a', 'b'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'multiply', 'description': 'Multiply a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int', 'parameters': {'type_': 6, 'description': 'Multiply a and b.\\n\\nArgs:\\n    a: first int\\n    b: second int', 'properties': {'a': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}, 'b': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['a', 'b'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'divide', 'description': 'Divide a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int', 'parameters': {'type_': 6, 'description': 'Divide a and b.\\n\\nArgs:\\n    a: first int\\n    b: second int', 'properties': {'b': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}, 'a': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['a', 'b'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'add', 'description': 'Adds a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int', 'parameters': {'type_': 6, 'description': 'Adds a and b.\\n\\nArgs:\\n    a: first int\\n    b: second int', 'properties': {'b': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}, 'a': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['a', 'b'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'multiply', 'description': 'Multiply a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int', 'parameters': {'type_': 6, 'description': 'Multiply a and b.\\n\\nArgs:\\n    a: first int\\n    b: second int', 'properties': {'a': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}, 'b': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['a', 'b'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'divide', 'description': 'Divide a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int', 'parameters': {'type_': 6, 'description': 'Divide a and b.\\n\\nArgs:\\n    a: first int\\n    b: second int', 'properties': {'b': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}, 'a': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['a', 'b'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'add', 'description': 'Adds a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int', 'parameters': {'type_': 6, 'description': 'Adds a and b.\\n\\nArgs:\\n    a: first int\\n    b: second int', 'properties': {'b': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}, 'a': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['a', 'b'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'multiply', 'description': 'Multiply a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int', 'parameters': {'type_': 6, 'description': 'Multiply a and b.\\n\\nArgs:\\n    a: first int\\n    b: second int', 'properties': {'a': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}, 'b': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['a', 'b'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'divide', 'description': 'Divide a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int', 'parameters': {'type_': 6, 'description': 'Divide a and b.\\n\\nArgs:\\n    a: first int\\n    b: second int', 'properties': {'b': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}, 'a': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['a', 'b'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'add', 'description': 'Adds a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int', 'parameters': {'type_': 6, 'description': 'Adds a and b.\\n\\nArgs:\\n    a: first int\\n    b: second int', 'properties': {'b': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}, 'a': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['a', 'b'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'multiply', 'description': 'Multiply a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int', 'parameters': {'type_': 6, 'description': 'Multiply a and b.\\n\\nArgs:\\n    a: first int\\n    b: second int', 'properties': {'a': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}, 'b': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['a', 'b'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'divide', 'description': 'Divide a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int', 'parameters': {'type_': 6, 'description': 'Divide a and b.\\n\\nArgs:\\n    a: first int\\n    b: second int', 'properties': {'b': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}, 'a': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['a', 'b'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3prxDSL5F8z",
        "outputId": "9fd5ffaa-6758-430c-c2c0-0139618b5723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Add 3 and 4. Multiply the output by 2. Divide the output by 5\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "['\\n', '\\n', '\\nThe answer is 2.8. \\n']\n",
            "Tool Calls:\n",
            "  divide (6dc946d7-0ee4-450a-b792-9c25aa3e0c41)\n",
            " Call ID: 6dc946d7-0ee4-450a-b792-9c25aa3e0c41\n",
            "  Args:\n",
            "    a: None\n",
            "    b: 5.0\n",
            "  add (c79bae18-522a-4673-ace5-4749013fa99e)\n",
            " Call ID: c79bae18-522a-4673-ace5-4749013fa99e\n",
            "  Args:\n",
            "    b: 4.0\n",
            "    a: 3.0\n",
            "  multiply (366e547a-d671-42db-8098-02f62cb6beae)\n",
            " Call ID: 366e547a-d671-42db-8098-02f62cb6beae\n",
            "  Args:\n",
            "    a: None\n",
            "    b: 2.0\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: divide\n",
            "\n",
            "Error: 1 validation error for divide\n",
            "a\n",
            "  Input should be a valid integer [type=int_type, input_value=None, input_type=NoneType]\n",
            "    For further information visit https://errors.pydantic.dev/2.9/v/int_type\n",
            " Please fix your mistakes.\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: add\n",
            "\n",
            "7\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: multiply\n",
            "\n",
            "Error: 1 validation error for multiply\n",
            "a\n",
            "  Input should be a valid integer [type=int_type, input_value=None, input_type=NoneType]\n",
            "    For further information visit https://errors.pydantic.dev/2.9/v/int_type\n",
            " Please fix your mistakes.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  add (6cc637cb-8498-4a65-997a-d47afe31590e)\n",
            " Call ID: 6cc637cb-8498-4a65-997a-d47afe31590e\n",
            "  Args:\n",
            "    b: 4.0\n",
            "    a: 3.0\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: add\n",
            "\n",
            "7\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  multiply (00c95069-d42f-4dec-9bcb-1be982dd1a03)\n",
            " Call ID: 00c95069-d42f-4dec-9bcb-1be982dd1a03\n",
            "  Args:\n",
            "    a: 7.0\n",
            "    b: 2.0\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: multiply\n",
            "\n",
            "14\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  divide (3d03769d-252c-4ba9-9c42-1288061ffb88)\n",
            " Call ID: 3d03769d-252c-4ba9-9c42-1288061ffb88\n",
            "  Args:\n",
            "    b: 5.0\n",
            "    a: 14.0\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: divide\n",
            "\n",
            "2.8\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The answer is 2.8.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** In above error the problem is that multiple chained operations (\"add\", \"multiply\", and \"divide\") are being interpreted incorrectly, likely leading to premature calls to tools without resolving earlier steps as model is following <b>DMAS</b> rule here. So we will improved the prompt and try again"
      ],
      "metadata": {
        "id": "vPH8DmZR5UZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [HumanMessage(content=\"First Add 3 and 4 then multiply answer with 2 and then divide answer with 5. Dont perform division initially\")]\n",
        "messages = react_graph.invoke({\"messages\": messages})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGcgxz7P5Mif",
        "outputId": "e5a825f7-c97b-4df4-adde-892b069b9d62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'add', 'description': 'Adds a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int', 'parameters': {'type_': 6, 'description': 'Adds a and b.\\n\\nArgs:\\n    a: first int\\n    b: second int', 'properties': {'b': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}, 'a': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['a', 'b'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'multiply', 'description': 'Multiply a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int', 'parameters': {'type_': 6, 'description': 'Multiply a and b.\\n\\nArgs:\\n    a: first int\\n    b: second int', 'properties': {'a': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}, 'b': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['a', 'b'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'divide', 'description': 'Divide a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int', 'parameters': {'type_': 6, 'description': 'Divide a and b.\\n\\nArgs:\\n    a: first int\\n    b: second int', 'properties': {'b': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}, 'a': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['a', 'b'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'add', 'description': 'Adds a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int', 'parameters': {'type_': 6, 'description': 'Adds a and b.\\n\\nArgs:\\n    a: first int\\n    b: second int', 'properties': {'b': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}, 'a': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['a', 'b'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'multiply', 'description': 'Multiply a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int', 'parameters': {'type_': 6, 'description': 'Multiply a and b.\\n\\nArgs:\\n    a: first int\\n    b: second int', 'properties': {'a': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}, 'b': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['a', 'b'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'divide', 'description': 'Divide a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int', 'parameters': {'type_': 6, 'description': 'Divide a and b.\\n\\nArgs:\\n    a: first int\\n    b: second int', 'properties': {'b': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}, 'a': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['a', 'b'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'add', 'description': 'Adds a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int', 'parameters': {'type_': 6, 'description': 'Adds a and b.\\n\\nArgs:\\n    a: first int\\n    b: second int', 'properties': {'b': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}, 'a': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['a', 'b'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'multiply', 'description': 'Multiply a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int', 'parameters': {'type_': 6, 'description': 'Multiply a and b.\\n\\nArgs:\\n    a: first int\\n    b: second int', 'properties': {'a': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}, 'b': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['a', 'b'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'divide', 'description': 'Divide a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int', 'parameters': {'type_': 6, 'description': 'Divide a and b.\\n\\nArgs:\\n    a: first int\\n    b: second int', 'properties': {'b': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}, 'a': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['a', 'b'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'add', 'description': 'Adds a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int', 'parameters': {'type_': 6, 'description': 'Adds a and b.\\n\\nArgs:\\n    a: first int\\n    b: second int', 'properties': {'b': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}, 'a': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['a', 'b'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'multiply', 'description': 'Multiply a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int', 'parameters': {'type_': 6, 'description': 'Multiply a and b.\\n\\nArgs:\\n    a: first int\\n    b: second int', 'properties': {'a': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}, 'b': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['a', 'b'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'divide', 'description': 'Divide a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int', 'parameters': {'type_': 6, 'description': 'Divide a and b.\\n\\nArgs:\\n    a: first int\\n    b: second int', 'properties': {'b': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}, 'a': {'type_': 3, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['a', 'b'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cALk58M15Ypu",
        "outputId": "f7cd753a-dc22-4f52-8b43-ee1e8b2e343c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "First Add 3 and 4 then multiply answer with 2 and then divide answer with 5. Dont perform division initially\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  add (45777f5d-804a-44c4-a651-948e69403e56)\n",
            " Call ID: 45777f5d-804a-44c4-a651-948e69403e56\n",
            "  Args:\n",
            "    a: 3.0\n",
            "    b: 4.0\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: add\n",
            "\n",
            "7\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  multiply (4c2b2a00-bb2a-480d-804f-ac6996b56f6d)\n",
            " Call ID: 4c2b2a00-bb2a-480d-804f-ac6996b56f6d\n",
            "  Args:\n",
            "    a: 7.0\n",
            "    b: 2.0\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: multiply\n",
            "\n",
            "14\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  divide (700e7bac-8bfd-4ff1-8adb-0a5d033810ee)\n",
            " Call ID: 700e7bac-8bfd-4ff1-8adb-0a5d033810ee\n",
            "  Args:\n",
            "    b: 5.0\n",
            "    a: 14.0\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: divide\n",
            "\n",
            "2.8\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The final answer is 2.8.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangSmith\n",
        "\n",
        "We can look at traces in LangSmith."
      ],
      "metadata": {
        "id": "17xYnFHf5oEd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_ynlXdz75cO7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}